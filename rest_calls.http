@bqa_host = https://mw04taq9j7.execute-api.ap-south-1.amazonaws.com/question_allocator_lambda_gw_stage

### call /invoke endpoint

POST {{bqa_host}}/invoke HTTP/1.1
content-type: application/json

{
	"questions": [
		"what is wildcard ?",
		"when to use concurrent hashmap ?"
	],
	"subject": "Java Programming",
	"stream": "Computer Science",
	"chapters": [
		"generics",
		"collections",
		"Threads",
		"Concurrency"
	]
}

### call /invoke endpoint with custom model

POST {{bqa_host}}/invoke?llm_model_id=meta&llm_model_version=llama3-70b-instruct-v1:0 HTTP/1.1
content-type: application/json

{
	"questions": [
		"what is wildcard ?",
		"when to use concurrent hashmap ?"
	],
	"subject": "Java Programming",
	"stream": "Computer Science",
	"chapters": [
		"generics",
		"collections",
		"Threads",
		"Concurrency"
	]
}

### call /invoke endpoint with custom model and debug mode on

POST {{bqa_host}}/invoke?llm_model_id=meta&llm_model_version=llama3-70b-instruct-v1:0&debug=true HTTP/1.1
content-type: application/json

{
	"questions": [
		"what is wildcard ?",
		"when to use concurrent hashmap ?"
	],
	"subject": "Java Programming",
	"stream": "Computer Science",
	"chapters": [
		"generics",
		"collections",
		"Threads",
		"Concurrency"
	]
}

### call /invoke/raw endpoint

POST {{bqa_host}}/invoke/raw HTTP/1.1
content-type: text/plain

"Apple is in talks with Google for using Gemini to bring generative AI features to iPhones"